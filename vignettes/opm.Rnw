\documentclass[nojss]{jss}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% GENERAL HINTS: STYLE OF THIS DOCUMENT
%
% Use \pkg{} for package names.
% Use \proglang{} for classes.
% Use \code{} for R commands.
% Use `` and '' for opening and closing double quotes, respectively.
% Put a single sentence in a single line, i.e. start a new line for each new sentence.
% Use empty lines for indicating paragraphs.
% Put several empty lines before the start of a new section.
% Put several empty lines surrounding a figure caption.
% Put words such as "via" in italics.
% Put taxon names in italics (but not strain names).
% Put TODOs in \rcsmark or \rcscom tags and remove them once done.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Add Revision Control System (RCS) Markups
\usepackage{ulem}      % use this for sout
\normalem              % set \emph to \textit again
% Additions (in blue)
\def\rcsadd#1{{\color{blue}#1}}
% Deletions (in red)
\def\rcsdel#1{{\color{red}\sout{#1}}}
% Yellow box (few words)
\def\rcsmark#1{\colorbox{yellow}{#1}}
%Yellow box (paragraph)
\def\rcscom#1{\noindent\newline\vspace*{0.5cm}\colorbox{yellow}{\parbox{\textwidth}{#1}}\vspace*{0.5cm}}

%% needed for CRAN checking
%\VignetteIndexEntry{Using opm}

% authors, first page
\author{Lea A.I. Vaas\\CBS-KNAW Fungal Biodiversity Centre \And
        Johannes Sikorski\\Leibniz Institute DSMZ  \AND
        Benjamin Hofner\\Universit\"at Erlangen-N\"urnberg \And
        Nora Buddruhs\\Leibniz Institute DSMZ \And
        Anne Fiebig\\Leibniz Institute DSMZ \AND
        Hans-Peter Klenk\\Leibniz Institute DSMZ \And
        Markus G\"oker\\Leibniz Institute DSMZ}

% title, first page
\title{\pkg{opm}: An R Package for Analysing OmniLog\textregistered \ Phenotype MicroArray Data}

% authors, header on every 2nd page
\Plainauthor{L.A.I. Vaas, J. Sikorski, B. Hofner, N. Buddruhs, A. Fiebig, H.-P. Klenk, M. G\"{o}ker}

% main title, first page
\Plaintitle{opm: An R package for analysing OmniLog\textregistered \ Phenotype MicroArray Data}

% short title, header on every 2nd page
\Shorttitle{Phenotype MicroArray Data}

\Abstract{
The OmniLog\textregistered \ Phenotype Microarray system is able to monitor simultaneously, on a longitudinal time scale, the phenotypic reaction of single-celled organisms such as bacteria, fungi, and animal cell cultures to up to 2,000 environmental challenges spotted on sets of 96-well microtiter plates.
The phenotypic reactions are recorded as respiration kinetics with a shape comparable to growth curves.
Tools for storing the curve kinetics, aggregating the curve parameters, recording associated metadata of organisms and experimental settings as well as methods for analysing these highly complex data sets graphically and statistically are increasingly in demand.

The \pkg{opm} \proglang{R} package facilitates management, visualization and statistical analysis of Phenotype Microarray data.
Raw measurements can be easily input into \proglang{R}, combined with relevant meta-information and accordingly analysed.
The kinetics can be aggregated by estimating curve parameters using several methods.
Some of them have been specifically adapted for obtaining robust parameter estimates from Phenotype Microarray data.
Containers of \pkg{opm} data can easily be queried for and subset by using the integrated metadata and other information.
The raw kinetic data can be displayed with customized plotting functions.
In addition to 95\% confidence plots and enhanced heat-map graphics for visual comparisons of the estimated curve parameters, the package includes customized functionality for user-defined simultaneous multiple comparisons of group means.
It is also possible to discretize the curve parameters and to export them for reconstructing character evolution or inferring phylogenies with external programs.
Tabular and textual summaries suitable for, e.g., taxonomic journals can also be automatically created and customized.
Export and import in the \proglang{YAML} markup language facilitates the data exchange among labs.
All functionality is exemplified using real-world data sets that are part of the package.
}

\Keywords{Bootstrap, Cell Lines, \pkg{grofit}, Growth Curves, \pkg{lattice}, Metadata, Microbiology, Respiration Kinetics, Splines, \proglang{YAML}}

\Plainkeywords{bootstrap, cell lines, grofit, growth curves, lattice, metadata, microbiology, respiration kinetics, splines, YAML}

\Address{
  Markus G\"{o}ker\\
  Leibniz Institute DSMZ -- German Collection of Microorganisms and Cell Cultures\\
  Braunschweig\\
  \\
  Telephone: +49/531-2616-272\\
  Fax: +49/531-2616-237\\
  E-mail: \email{markus.goeker@dsmz.de}\\
  URL: \url{www.dsmz.de}\\
}


%% this must be included if Sweave is used (with % symbols):
%% need no \usepackage{Sweave.sty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

% NOTE: Rstudio might insert a line '\SweaveOpts{concordance=TRUE}' after '\begin{document}', which is removed again by the build script, as it causes warning messages during R CMD check.
% So please don't try to rescue that line, and don't rely on it being there.

%% so far adding a TOC proved unsuccessful -- apparently the JSS style doesn't allow for a TOC
%\tableofcontents
%\newpage


\section[Introduction]{Introduction}\label{introduction}

\subsection[Preamble for impatient readers]{Preamble for impatient readers}\label{preamble}

Readers who want to jump right into examples for applying \pkg{opm} to their data will find an overview of what the package can do for them in Figure~\ref{fig:work_flow}.
Next, Figure~\ref{fig:overview} should be looked at, as it lists the names of the functions that can be used in each step of the \pkg{opm} work flow.
Examples for each step would then be found in the according subsections of of Section~\ref{application}.
Details on the scientific background could well be skipped during a first reading.
The user would nevertheless find them in Section~\ref{methods}, including references for important functionality.
\rcscom{
TODO MG: Add link to new resources section.
}


\subsection[Scientific introduction]{Scientific introduction}\label{scientific introduction}

The phenotype is regarded as the set of all types of traits of an organism \citep{mahner}.
The phenotype is of high biological relevance, as it is the phenotype which is the object of selection and, hence, is the level at which evolutionary directions are governed by adaptation processes \citep{mayr}.
It is also the phenotype which is of direct relevance to humans, for example in exploiting microorganisms for industrial purposes or in the combat of pathogenic organisms \citep{broadbent, mithani}.
In the study of single-cell living beings, such as bacteria, fungi, plant or animal cells, it is an important field of research to study the phenotype by measuring physiological activities as a response to environmental challenges.
These can be single carbon sources, which may be utilized as nutrients and hence trigger cellular respiration, or substances such as antibiotics, which may slow down or even inhibit cellular respiration, indicating a successful inhibitory effect on potentially pathogenic organisms.
The intensity of cellular respiration correlates with the production of NADH engendering a redox potential and thus a flow of electrons in the electron transport chain.
To measure cellular respiration in an experimental assay, this flow of electrons can be utilized to reduce a tetrazolium dye such as tetrazolium violet, thereby producing purple colour \citep{bochnersava}.
In principle, the more intense the colour, the larger the physiological activity.


% The following figure is first referred to in the section 'Introduction'.
% Make sure it remains mentioned there.
\begin{figure}
\scalebox{1.225}{
\includegraphics{opm_fig_1}
}
\caption{\label{fig:data}
Overview of the data assembly from an PM experiment and the possible additions using \pkg{opm}.
The raw colour-formation values result in sets of 96 raw kinetics per plate.
Using \pkg{opm}, they can be augmented by the information coded in the shape characteristics.
This yields 96 sets of parameters per plate, each containing four robustly estimated parameters that describe distinct aspects of the respective curve shape.
The \pkg{opm} package also offers tools for further combining this bundle of raw, aggregated and also discretized data of each single kinetic with meta-information on the organisms and/or experiments.
Based on this meta-information, a variety of visual and statistical comparison tools for either the raw or the aggregated data are available in \pkg{opm}.
}
\end{figure}

The Phenotype MicroArray (PM) system is capable of measuring a large number of phenotypes in a high-throughput system that uses such as tetrazolium detection approach.
About 2,000 distinct physiological challenges, such as the metabolism of single carbon sources for energy gain, the metabolism under varying osmolyte concentrations, and the response to varying growth-inhibitory substances are included in the PM microtiter plates \citep{bochner2001, bochner2009}.
The OmniLog\textregistered \ PM system records the colour formation in an automated setting (every 15 minutes) throughout the duration of the experiment, which may last up to several days.
Thus the experimenter ends up with high-dimensional sets of longitudinal data, the PM respiration kinetics.
For a detailed introduction into the experimental setup for obtaining OmniLog\textregistered \ PM respiration kinetic data we refer to the OmniLog\textregistered \ website (http://www.biolog.com/) and the associated hardware and software manuals.
Briefly, 96-well microtiter plates with substrates, dye, and bacterial cells are loaded into the OmniLog\textregistered \ reader, a hardware device which provides the appropriate incubation conditions and also automatically reads the intensity of colour formation during tetrazolium reduction.
The OmniLog\textregistered \ reader is driven by the \textit{Data Collection} software.
The stored results files, which are in a proprietary format, are then imported into the \textit{Data Management}, \textit{File Management/Kinetic Analysis}, and \textit{Parametric Analysis} software packages for data analysis.

In the case of positive reactions, the kinetics are expected to appear as (more or less regularly) sigmoidal curves in analogy to typical bacterial growth curves (Figure~\ref{fig:data}).
The intrinsic higher level of data complexity contains additional valuable biological information which can be extracted by exploring the shape characteristics of the recorded curves \citep{bisbin}.
These curve features can, in principle, unravel fundamental differences or similarities in the respiration behaviour of distinct organisms, which cannot be identified by the traditional end-point measurements alone.
But the meta-information of interest on the organisms and experimental conditions must also be available for an according statistical assessment.

The motivation for the here presented \pkg{opm} package originated from (i) the need to overcome the limited graphical and analysis functions of the proprietary OmniLog\textregistered \ PM software and (ii) the desirability of an analysis system for this kind of data in a free statistical software environment such as \proglang{R} \citep{r}.
At the moment, the visualisation of the kinetics by the proprietary OmniLog\textregistered \ PM software is of limited quality, especially when simultaneously comparing the curves from more than two experiments.
Its calculation of curve parameters is rather crude \citep{vaas, biologInc}.
The statistical treatment of raw kinetic data and curve parameters would involve cumbersome manual and hence error-prone manipulations of data in typical spreadsheet applications before they may be imported into appropriate statistical software.
Finally, the amount of organismic or experimental metadata that can be added to the raw data is extremely limited.

Based on a previous study \citep{vaas} the here presented \pkg{opm} package offers functionalities for a fast, robust and comprehensive evaluation of PM respiration kinetics suitable for a wide range of experimental questions.

Using customized input functions, raw kinetic data can be transferred into \proglang{R}, stored as \proglang{S4} objects \citep{chambers} containing single or multiple OmniLog\textregistered \ PM plates and further processed.
The package features the statistically robust calculation and attachment of aggregated curve parameters including their (bootstrapped) confidence intervals.
Moreover, infrastructure is provided to merge this with any kind of additional metadata.
These complex data bundles can then be exported in \proglang{YAML} format (http://www.yaml.org/), which is a human-readable data serialization format that can be read by most common programming languages and facilitates fast and easy data exchange between laboratories.
As \pkg{opm} is also able to generate \proglang{R} matrices and data frames, output in CSV (character-separated values) files would also be easy.

The framework for data evaluation starts with several functions for the graphical display of the data such as the raw respiration curve kinetics or the confidence intervals of aggregated curve parameters.
With sophisticated selection methods the user is able to sort, group and arrange the data according the specific experimental questions in the plotting and analysis framework.
Since most addressed experimental questions require to statistically compare not only single curves, but distinct groups of curves, the package provides adapted functionality for performing simultaneous multiple comparisons of group means \citep{bretz}.
Because the definition of groups using stored metadata is highly flexible, the user is enabled to compute contrast tests for individually defined sets of mean comparisons \citep{hsu}.

For further specific graphical or statistical analysis according to the needs of the users, the \pkg{opm} package organises and maintains the data such that any additional data exploration using other packages in the \proglang{R} environment are easily applicable.

The work flow described below includes the input of raw kinetic data and integration of corresponding metadata, conversion into suitable storage formats, the computation of a set of four parameters sufficient for comprehensively describing the curves' shape (aggregated data), manipulating and querying the constructed objects, visualizing both raw kinetics and aggregated data, statistical comparison of group means, discretization of the curve parameters and corresponding export methods and obtaining additional information the substrates.

\section[Methods]{Methods}\label{methods}

\subsection[Overview]{Overview}\label{methods overview}

In the following the work flow (see Figure~\ref{fig:work_flow}) for generating an \proglang{R} object that contains the kinetic raw data from one to several OmniLog\textregistered \ plates along with the corresponding metadata of interest, and optionally the aggregated and potentially also discretized curve parameters, is described.
It is explained how to analyse either raw data, metadata, aggregated data (curve parameters), or combinations of all of them, as stored in the respective \proglang{R} objects, by graphical and/or statistical approaches.

% The following figure is first referred to in the section 'Overview'.
% Make sure it remains mentioned there.
\begin{figure}
\scalebox{1.225}{
\includegraphics{opm_fig_2}
}
\caption{\label{fig:work_flow}
A depiction of the work flow possible within \pkg{opm} and its potential interplay with base \proglang{R}, add-on packages for \proglang{R} and third-party software.
The work flow allows the user full flexibility with respect to the type of information added to the created \proglang{R} objects and to the order of steps in which this is achieved.
See Figure~\ref{fig:overview} for the functions that can be used in the respective steps.
}
\end{figure}


The raw kinetic data can be exported by the proprietary OmniLog\textregistered \ software \textit{File Management/Kinetic Analysis} as CSV files and imported into the \pkg{opm} package using \code{read\_opm()}. This is explained in detail in section Section~\ref{methods import}, whereas according code examples is found in Section~\ref{application import}.

To facilitate a comprehensive and straightforward data processing and analysis, the raw and aggregated data of each single kinetic can be concatenated and combined with metadata (see Figure~\ref{fig:data}). The underlying principles are described in Section~\ref{methods metadata}, whereas example code for metadata management is included in Section~\ref{application metadata}.

In a further step the valuable biological information coded in the shape characteristics of the recorded curves can be extracted.
Four descriptive curve parameters are estimated, which is explained in detail in Section~\ref{methods aggregating}, whereas example code for metadata management is included in Section~\ref{application aggregating}.

\rcscom{
The other forthcoming steps and subsections should also be mentioned and linked here.
}

The work flow of the package was designed for offering a maximum of flexibility with respect to the type of information added to the \proglang{R} object and to the order of steps in which this is achieved.
For example, it is possible to add first the metadata and to perform some of the later described analysis and second to aggregate the raw kinetics and go on with analysis of the aggregated values.
Discretization might frequently not be of interest because it loses information.
Since experimental frameworks can be imagined where only very limited meta-information is available, it is also feasible to work without metadata at all.

\subsection[Data import]{Data import}\label{methods import}

The proprietary OmniLog\textregistered \ PM data analysis software \textit{File Management/Kinetic Analysis} \citep{biologInc} can export the kinetic raw data from single or multiple plates as CSV files.
These contain a small amount of associated run information that has been entered at the interface of the OmniLog\textregistered \ PM \textit{Data Collection} software, which controls the OmniLog\textregistered \ reader.
Currently this generation of CSV files involves the creation of intermediary files with the extension ``d5e'' from the original ones with the extension ``oka''.
For use with \pkg{opm}, the raw kinetic data should be exported into a single CSV file for each measured plate.
The \pkg{opm} package currently does not support the input of several plates from PM-mode runs stored in a single CSV file, but it offers the function \code{split\_files()} for splitting old-style CSV files containing multiple plates.
(We refer to the CSV exports from the currently distributed OmniLog\textregistered \ PM \textit{File Management/Kinetic Analysis} software as ``old style''.
Forthcoming versions are expected to export the data in a slightly different CSV format we call ``new style''.
Please contact your local representative of the vendor for the latest software version.)

As of version 0.4-0, \pkg{opm} also supports the input of MicroStation\textregistered \ CSV files (frequently used in conjunction with EcoPlate\textregistered \ assay for microbial community analysis).
These files contain only end-point measurements but potentially several plates, which can nevertheless be input together with their potentially also rich meta-information.

The easiest way to load the raw kinetic data (as CSV files or as \proglang{YAML}) into \proglang{R} in a single step is using the function \code{read\_opm()} (see Figure~\ref{fig:work_flow}).
If raw data from only one single-plate OmniLog\textregistered \ PM are imported, the resulting object belongs to the \proglang{S4} class \proglang{OPM}.
This class for holding single-plate OmniLog\textregistered \ PM data originally only includes the (limited) meta-information read from the original input CSV files, but an arbitrary amount of metadata can be added later on (see Figure~\ref{fig:work_flow}).
If multiple plates are imported, the resulting object automatically belongs to the \proglang{S4} class \proglang{OPMS}.
In the \proglang{OPMS} class, data may have been obtained from distinct organisms and/or replicates, but must correspond to the same plate type and must contain the same wells (see Figure~\ref{fig:work_flow}).
The function \code{read\_opm()} has an argument ``convert'' which controls how sets of plates with distinct types are treated; for instance, the function can return a list of  \proglang{OPMS} objects, one for each encountered plate type.

The entire \proglang{S4} class hierarchy used by \pkg{opm} is shown in Figure~\ref{fig:class_hierarchy}.
Users come in direct contact only with the \proglang{OPM}, \proglang{OPMA}, \proglang{OPMD} and \proglang{OPMS} classes.
A number of \proglang{S3} helper classes are also used by several functions.


\subsection[Integration of metadata]{Integration of metadata}\label{methods metadata}

The interface of the \textit{Data Collection} software of the OmniLog\textregistered \ reader is restricted in size and contains only comparatively few fields entering accompanying information  to each plate such as on the organism under study and the culture conditions
Further, not all of these fields are exported together with the raw measurements.
But for most experimental designs is is clearly necessary to add much more meta-information to the kinetic data.
It has to be emphasized that metadata can include all kind of describing characteristics of the observed organism(s) such as taxonomic affiliation and geographical and/or ecological origin, or of the performed experimental setting such as culture conditions, genetic modifications, physiological information of any kind and so on.

To this end, the \pkg{opm} user can integrate the metadata in \proglang{OPM} and \proglang{OPMS} objects using the function \code{include\_metadata()} (among other functions for this task; see Figure~\ref{fig:work_flow}).
Usually, the metadata are kept in a data frame which can be saved to, and generated from, a CSV file.
For an unambiguous match between the raw kinetic data in the \proglang{OPMS} object and the collected metadata, a unique identifier is needed.
By default the combination of \textit{Setup Time} and \textit{Position} is used, which should unequivocally identify certain plates.
\textit{Setup Time} indicates the date and time at the precision of seconds of starting the batch read in the OmniLog\textregistered \ reader.
\textit{Position} indicates the position of the plate in the OmniLog\textregistered \ reader.
(For instance, \textit{10-A} indicates the plate sliding carriage number 10 in slot A of the reader, but for \pkg{opm} the meaning is irrelevant, as these entries only serves as identifiers.)
Both \textit{Setup Time} and \textit{Position} are automatically recorded by the OmniLog\textregistered \ reader \textit{Data Collection} software and are exported by the OmniLog\textregistered \ PM \textit{File Management/Kinetic Analysis} software into CSV files together with the raw kinetic data.

To facilitate the compilation of metadata information, \code{collect\_template()} generates a data frame (and additionally, if requested, a CSV file) in which each line represents a single PM plate.
The function \code{collect\_template()} automatically includes the \textit{Setup Time} and \textit{Position} (or any other CSV data of interest) of each plate into the data frame or file.
The user can subsequently add further columns describing any metadata of interest of any PM plate of interest.
The data frame or CSV file can then be queried for the information specific to each plate, and the resulting data integrated into \proglang{OPM} or \proglang{OPMS} objects using \code{include\_metadata()}.
Whereas this function will usually result in non-nested metadata entries, the implementation allows one, in principle, to deal with arbitrarily nested meta-information.
The amount of meta-information added and plates analysed is only limited by the available computer memory.


% The following figure is already referred to in the section 'Data import'.
% Make sure it remains mentioned there.
\begin{figure}
\scalebox{1.225}{
\includegraphics{opm_fig_3}
}
\caption{\label{fig:class_hierarchy}
This picture shows the \proglang{S4} class hierarchy used by \pkg{opm}.
Class names are shown in bold within the boxes.
Boxes with dark background indicate virtual classes, those with light background indicate real classes whose objects can be created and manipulated by some code.
Arrows indicate either inheritance relationships (pointing from the parent to its child class) or object composition (pointing from the container class to its element class).
Note particularly that \proglang{OPM}, which only contains raw data, csv data and metadata, is the parent class of \proglang{OPMA}, which also contains aggregated data (and has methods for dealing with them).
\proglang{OPMD} inherits from \proglang{OPMA} and stores discretized curve parameters in addition to aggregated values.
\proglang{OPMS} is a container class that holds \proglang{OPM}, \proglang{OPMA} and/or \proglang{OPMD} objects.
These can usually co-occur in a single \proglang{OPMS} object but for some calculations the additional information in \proglang{OPMA} or \proglang{OPMD} objects is strictly required.
The query functions \code{has\_aggr()} and \code{has\_disc()} are available for checking from which kinds of objects an \proglang{OPMS} is composed.
See the \pkg{opm} manual and Section~\ref{application} for further details.
The non-virtual classes in the upper part of the figure are either well-known in \proglang{R} (matrices and arrays) or not directly manipulated by the user (\proglang{CMAT}).
}
\end{figure}


The user can provide additional information to the metadata data frame on the fly (if not provided in CSV) by calling the function \code{edit()}, which opens the \proglang{R} editor enabling the user to modify and add data.
Beside changing the metadata entries by using the \proglang{R} Editor, the function \code{map\_metadata()} offers a secure way to map metadata within \proglang{OPMS} objects.
The replacement function \code{metadata()<-} enables the user to set the entire meta-information, or specific entries, directly.


\subsection[Batch conversion of many files]{Batch conversion of many files}\label{methods batch}

To process and store huge numbers of raw data files, the function \code{batch\_opm\_to\_yaml()} reads all OmniLog\textregistered \ CSV files (or \proglang{YAML} files previously generated with \pkg{opm}) within a given list of files and/or directories and converts them to \pkg{opm} \proglang{YAML} format.
It is possible to let \pkg{opm} automatically include metadata and aggregated values (curve parameters) during this conversion.
File selection and unselection using regular expressions or globbing patterns is integrated in the function.
The result from each file conversion is reported in detail, and a \textit{demo} mode is available for viewing the attempted file selections and conversions before actually running the (potentially time consuming) conversion process.
The package is accompanied by a command-line script \code{run_opm.R}, enabling the users to run the batch conversion without starting an interactive \proglang{R} session.


\subsection[Aggregating data by estimating curve parameters]{Aggregating data by estimating curve parameters}\label{methods aggregating}


Descriptive curve parameters from the kinetic raw data can be calculated and included in \proglang{OPM} and \proglang{OPMS} objects using the function \code{do\_aggr()}.
Curve parameters can be extracted using a spline-based fitting procedure implemented in \pkg{opm}.
(Extraction of curve parameters through the fit of sigmoid functions proved for several PM curve shapes to yield biologically unrealistic values \citep{vaas} and have therefore not been implemented.)
Three different modelling alternatives for the splines exist:
(low-rank) cubic smoothing splines \citep{Reinsch:1967} as implemented in \code{smooth.spline} from the  \pkg{base} package, thin plate splines \citep[a generalization of smoothing splines]{Wood2003} and P-splines \citep{eilers}.
The latter two are implemented in the package \pkg{mgcv}.
% mgcv seems to already be cited, as Wood2003 is lited among the package citations on CRAN
It is also possible to access methods from the package \pkg{grofit} \citep{kahm} or to use a native  implementation which is faster but only estimates two of the four parameters.
\rcscom{
Comment from Johannes: Why have these specific spline modelling options been chosen?
What is the difference, what are pros and cons of each method.
In short words: It would be good to add here some explanations so that the standard user gets an idea why and under which circumstances he should use which of the methods.
At moment, the method from grofit is the default method. 
Why this one?
Are the other methods implemented by Benjamin not better?
}

The descriptive curve parameters estimated by \pkg{opm} are shown in Figure~\ref{fig:parameters}.
The parameters $\lambda$, $\mu$, and A are derived by default from spline fits, whereas AUC is estimated \textit{via} numerical integration (see Figure 2 in \cite{kahm} for details).
%If desired, the user is free to use the parameter estimates from the provided model fits as well \citep{vaas}.
% not sure whether this works
In addition to the point estimates for the parameters from both model and spline, confidence limits can be calculated (for the spline-based approach \textit{via} bootstrapping), with 95\% being the default value \citep{efron}.
But confidence intervals and according group means can also be calculated from experimental repetitions, as explained in Section~\ref{methods plotting-aggregated}.
Attaching the aggregated data to an \proglang{OPM} object yields an object of the class \proglang{OPMA}, which can also be stored within an \proglang{OPMS} container object.


\begin{figure}
\scalebox{1.225}{
\includegraphics{opm_fig_4}
}
\caption{\label{fig:parameters}
A schematic depiction of a typical growth or respiration curve and the parameters estimated by \pkg{opm}.
The descriptive curve parameters are the lag phase $\lambda$,  the respiration rate $\mu$ (corresponding to the steepness of the slope), the maximum cell respiration A (corresponding to the maximum value of the curve) and the area under the curve AUC.
Note that many respiration curves, even if representing a clearly positive reaction, do not correspond to this idealized scheme.
The parameters can nevertheless be robustly estimated from deviating curves, particularly \textit{via} spline fits \citep{vaas}.
}
\end{figure}


\subsection[Manipulation of OPM and OPMS data]{Manipulation of \proglang{OPM} and \proglang{OPMS} data}\label{methods manipulation}


After integration of additional metadata \textit{via} \code{include\_metadata()} and adding aggregated curve parameters \textit{via} \code{do\_aggr()}, an \proglang{OPMA} or \proglang{OPMS} object comprises basically three pieces of information: (i) the kinetic raw data; (ii) the aggregated data, i.e., the curve parameters $\lambda$, $\mu$, A, and AUC and optionally their corresponding 95\% confidence limits; and (iii) the metadata.

As usual, the data analysis starts with data exploration for which the user may now wish to subset and query using these pieces of information.
As Figure~\ref{fig:overview} illustrates, the package provides methods for (i) querying and sub-setting \proglang{OPMS} objects, (ii) plotting the data in some customized manner, and (iii) converting the \proglang{OPM} or \proglang{OPMS} to other objects for an independent exploration by the user (discretization and exporting in some useful file formats is also possible).


\begin{figure}
\scalebox{1.225}{
\includegraphics{opm_fig_5}
}
\caption{\label{fig:overview}
This scheme provides a detailled overview of the possible strategies and appropriate functions for data analysis using the \pkg{opm} package.
Beginning with (a) CSV file(s) containing raw kinetic data exported by the proprietary OmniLog\textregistered \ software \textit{File Management/Kinetic Analysis}, the workflow depicts the functionalities for the diverse options to compile a \proglang{OPM(A)(D)(S)} data conctainer including additional metadata.
Additionally, methods for metadata management, plotting the data in a customized manner, querying and sub-setting \proglang{OPM(A)(D)(S)} objects, statistical comparison of multiple group means, and data conversion tools including discretization and output in files are provided.
}
\end{figure}


Furthermore, the bundled structure of an \proglang{OPMS} object, and the methods of the class, permit queries for the presence of a specific metadata key or a specific value of a specific metadata key, or a specific combination of values and/or keys, and also enable the user to subset an \proglang{OPMS} object accordingly.


\subsection[Plotting functions]{Plotting functions}\label{methods plotting}



\subsubsection[Plotting functions for raw data]{Plotting functions for raw data}

\rcscom{
Lea, a disadvantage of using subsubsections is that they are not numbered and cannot be referred to.
If this cannot be fixed (without generating other problems), please convert back to the previously used sections in all parts of this document.
Please also use label annotations and check for dead links.
}

The function \code{xy\_plot()} displays the raw measurements (y-axis) in dependency of the time (x-axis).
For each well one sub-panel is drawn, and the user is free to colour the plotted curves by either their affiliation to a specific plate or by a combination of (metadata-)variables of choice.
By default the panels are arranged according to the factual microtiter plate dimensions (eight rows labelled A to H $\times$ 12 columns labelled 01-12), but other user-defined arrangements are easily feasible because the plates can be subset by selecting specific wells.
Every panel is annotated with the microtiter plate numbering (A01 to H12) and additionally or alternatively with the substrate name (given the plate type, the \pkg{opm} package can translate all well coordinates to substrate names).
Thus, the function enables the user to compare the curve data in a customized and useful arrangement \citep{vaas}.

The function \code{level\_plot()} provides false-colour level plots from the raw respiration measurements over time.
Each respiration curve can be displayed as a thin horizontal line, in which the measured respiration value (OmniLog\textregistered \ units) is represented by colour, while the x-axes indicates the measurement times.
With increasing respiration measurement values, the displayed colour changes (by default) from light yellow into dark orange and brownish.
By default one sub-panel in the level-plot corresponds to one complete plate comprising 96 lines, but as in the case of \code{xy\_plot()} plotting could also be preceded by creating subsets of the plates.
The user can obtain an overview in a compacted design \citep{vaas}.
This plot offers a display format which is especially powerful in uncovering general differences between plates, for example longer lag-phases or smaller AUC values across the majority of wells.


\subsubsection[Plotting the aggregated data]{Plotting the aggregated data}\label{methods plotting-aggregated}

\rcscom{
The phrasing of this subsection appears to be fine for Johannes now. At moment, Johannes has now own experience on normalizations using \code{extract()}. If phrasing is also ok, for Lea, she should remove this note.
}

For the graphical representation of the aggregated data, namely point estimators and corresponding confidence limits for the curve parameters, the function \code{ci\_plot()} provides a framework to plot subsets of different parameters in a convenient and easily applicable manner.
This straightforward assembly of different curves' characteristics in a single overview facilitates the interpretation and comparison of user-defined data subsets arranged according to the technical and/or biological repetition structure or other aspects of the experimental design \citep{vaas}.

When analysing empirically obtained measurements such as PM data it is important to consider possible systematic variations and control for those.
For a PM experiment the purpose of such a normalization is to minimize systematic variations in the aggregated curve parameters so as to more easily recognize biological differences, as well as to allow for the comparison of parameters across plates processed in different experimental runs.
The underlying ideas are mainly derived from DNA-microarray experiments for measuring gene-expression levels \citep{quackenbush}.

With \code{extract()} the user is enabled not only to select certain aggregated or discretized values into common matrices or data frames, but also to apply normalizations before computing point estimates and their respective confidence intervals for individually defined experimental groups.
\rcsadd{Normalization indicates here (i) the subtraction of the mean of each plate from each of its values, or (ii) all values can be divided by the mean of the value. The details can be set in the respective arguments of \code{extract()}.}
, either per row or per column. Alternatively, 
Although this function is intended mainly as a helper function for \code{ci_plot()}, it can be quite useful for specific normalization purposes, for example when data were derived before and after servicing the Omnilog\textregistered \ facility, which might result in shifting the measurements by a certain amount.
In conjunction with \code{extract()}, \code{ci_plot()} allows for visualizing point estimates and confidence intervals of groups of parameter estimates.
For visualizing differences between groups and their confidence intervals, see \code{opm_mcp()} as described in Section~\ref{methods group-means}.
 
Additionally, the package offers the possibility of plotting the aggregated curve parameters as a heat map \textit{via} the function \code{heat\_map()}.
Heat maps appear particularly powerful for visualizing the outcomes of PM experiment because dendrograms inferred from both the substrates and the plates can be used to rearrange the plot.
Since the user is free to define which metadata or strain information are of interest for the annotation of the plot and the clustering analysis, this tool provides a powerful feature for data exploration in specialized contexts.
For instance, the naming scheme of the individual plates can be devised by selecting associated metadata; it is also possible to automatically construct row groups by selecting the same or other meta-information.
\code{heat\_map()} is mainly a wrapper for the \code{heatmap()} functions from either the \pkg{stats} or the \pkg{gplots} \proglang{R} package, but contains some useful adaptations to the PM data, facilitates the selection of a clustering algorithm and the construction of row and column groups, and provides more appropriate default solutions for row and column descriptions sizes (we suppose that in most situations the pictures produced by \code{heat\_map()} should not need manual adaptation in these respects).


\subsection[Statistical comparisons of group means]{Statistical comparisons of group means}\label{methods group-means}

\rcscom{
The phrasing of this subsection is fine for Johannes now. If phrasing is also ok, for Lea, she should remove this note.
}
Besides comparing single curves, the user may also be interested in statistically comparing the mean values of distinct groups of curves.
For example, imagine the comparison of four different bacteria using GEN-III microplates.
Assume that for each bacterial strain, ten replicates have been performed.
(An according example dataset is actually available in the \pkg{opmdata} package.)
Do these four bacteria differ in, e.g., the mean value of, e.g., curve parameter A of, e.g, well A1?
Here, a statistical comparison of four groups (four organisms) of each 10 values (curve parameter A of 10 replicates of well A1) would need to be performed.
Statistically, this results in simultaneous inferences across multiple questions \citep{hothorn}.
To address this issue the function \code{opm_mcp()} provides functionality for performing simultaneous multiple comparisons of group means by internally calling \code{glht()} from the \pkg{multcomp} package \citep{hothorn} but providing an easier interface for it, specifically adapted to the typical objects of \pkg{opm}.
By referring to available metadata and/or the substrate names, the user is able to define groups of interest (e.g., four different bacteria), set up a model of choice and perform multiple comparison of group means on individually specified contrasts \citep{bretz, hsu}.
The choice of appropriate models and contrasts will be explained in detail below.
As comparisons of the different curve parameters are performed separately, it is possible to ask very specific questions on differences between curve shapes.

At this point, it is necessary to highlight the power and flexibility of simultaneous multiple comparison procedures and encourage the user to apply contrast tests on individually designed sets of mean comparison rather than to employ the probably more popular classical ANOVA-based approaches followed by F-tests.
It has to be pointed out that in general F-tests \textit{only} provide global information about main effects and interaction effects.
That is, only the significance of a result yields evidence for a difference in the means among any of the considered treatments.
For example, in the framework of PM data, a significant F-test on the effect of the substrate would indicate that at least two of the substrates cause distinct respiration. 
Considering that each PM experiment encounters at least 96 different substrates (overall up to 2,000), this information is obviously nearly useless.
Moreover, F-tests neither provide information about effect sizes nor do they ease to address comparisons of particular interest \citep{schaarschmidt}.

From our point of view, the very most underlying experimental questions in PM experiments can be best expressed as a set of particular mean comparisons, resulting in a multiple-comparison problem \citep{hochberg}.
But if an increasing number of hypotheses is tested, with the number of true hypotheses unknown, the probability of at least one wrong testing decision also increases.
That is, if an increasing number of groups is compared to each other, conclusions on significant differences between a pair of groups are increasingly likely to be wrong.
Thus the so-called family-wise error-rate, which is essentially the probability of at least one false rejection among all the null hypotheses, needs to be controlled \citep{tukey}.
The here internally employed functionality from the package \pkg{multcomp} provides solutions for all listed difficulties, since it allows for testing a user-defined set of contrasts based on a broad range of model types while internally controlling the family-wise error-rate.  

\rcscom{
Lea \& Johannes, explain the implementation of how factors are merged to produce a pseudo-one-way-layout \citep{schaarschmidt}, and/or remove this note.
Needs to be done by Lea (I, Johannes, do not feel competent enough for that)
}
The function \code{opm\_mcp()} internally reshapes the data into a ``flat'' data frame containing one column for the chosen parameter value, one column for the well (substrate) name and optionally additional columns for the selected metadata.
For performing the testing procedure, a model has to be stated that specifies the factor levels that determine the grouping \citep{searle, hothorn}.
The \code{opm\_mcp()} function allows for applying such testing directly to \proglang{OPMS} objects.


\subsection[Discretizing and export for phylogenetic analysis]{Discretizing the aggregated data and export for phylogenetic analysis}\label{methods discrete-multistate}

Whereas the main data analysis strategies of the \pkg{opm} package are based on quantitative, continuous data (as described in the previous chapters), users may nevertheless be interested in discretizing the estimated curve parameters.
For instance, discretizing the data is necessary for analysing the data with external programs that cannot deal with continuous characters.
Indeed, phylogeny software such as \proglang{PAUP*} \citep{swoff} and \proglang{RAxML} \citep{stamatakis} is limited to at most 32 distinct character states (to the best of our knowledge, a maximum-parsimony algorithm applicable directly to continuous data has only been implemented in \proglang{TNT} \citep{goloboff}).
Phylogenetic studies of PM data are of interest because such phenotypic information is frequently used for taxonomic purposes in microorganisms, and here phylogenetic inference methods might be superior to clustering algorithms \citep{felsenstein}.
But tabular or textual descriptions of physiological reactions classified into negative, weak (ambiguous) and positive reactions (see next paragraph for details) are of even greater relevance in current microbial taxonomy \citep{tindall}.

The \pkg{opm} package includes data-transformation functionality within the \code{discrete()} methods for coding continuous characters by assigning them to a given number of equal-width categories within a given range.
For example, for the parameter A (the maximum curve height) the theoretically possible range between 0 and 400 OmniLog\textregistered \ units could be used.
The data should then be analysed under ordered (Wagner) maximum parsimony in \proglang{PAUP*} \citep{farris} or with the options for ordered multi-state phenotypic characters in \proglang{RAxML} \citep{berger}, or corresponding settings in other programs, to minimize the loss of information caused by discretizing the values.
For this reason, this kind of unsupervised, equal-width-intervals discretization \citep{dougherty,ventura}, even though simple, appears appropriate for this task.
In this context, it also makes not much sense to let a discretization method determine the number of categories because they are not dictated by some property of the data but by the limitations of the subsequently to apply analysis software.
The \pkg{opm} package offers appropriate functions for data export.

\subsection[Determining positive and negative reactions and displaying them as text or table]{Determining positive and negative reactions and displaying them as text or table}\label{methods discrete-binary}

If users were interested to discretize the parameters into ``positive'' and ``negative'' results, this would apparently make most sense for the parameter A because here it is not of interest when and how fast a reaction starts (which would be coded in $\lambda$ and $\mu$, respectively) or how much overall respiration was achieved (as coded in AUC) but whether or not a reaction takes place at all.
Unfortunately, PM data frequently result in a continuum of A values between clearly negative and clearly positive reactions.
For instance, the distribution of A in the example datasets distributed with the \pkg{opm} and \pkg{opmdata} packages is clearly bimodal, but contains a large number of intermediary values.
For this reason, the \code{discrete()} methods and their more user-friendly wrapper \code{do\_disc()} offer a gap-mode discretization by interpreting a given range of values (within the overall range of observations) as ``ambiguous''.
Values below would then be coded as negative, values above the range as positive, and values within the range as either missing information or an intermediary state, ``weak''.
This range could be determined by some discretization approach known from the literature \citep{dougherty,ventura}.

The \pkg{opm} package offers its automated determination using k-means partitioning as implemented in \pkg{Ckmeans.1d.dp} \citep{song2011}, using an exact algorithm for one-dimensional data.
Alternatively, an algorithm implemented in \code{best\_cutoff()} is available, but it requires measurement replicates (which are highly recommended, if not mandatory, anyway) which need to be specified in the metadata.
Both methods are accessible \textit{via} \code{do\_disc()}.
Export as richly annotated \proglang{HTML} table is possible using \code{phylo\_data()}.
If analysis with phylogenetic programs was of interest, in the case of an intermediary state the data should then be analysed as described above.
If intermediary values were coded as missing information they could be analysed under either Wagner or unordered (Fitch) maximum parsimony in \proglang{PAUP*} \citep{farris, fitch} or with the options for binary phenotypic characters in \proglang{RAxML} \citep{berger}, or corresponding settings in other programs.

%{\color{red} \subsection[Accessing substrate information]{Accessing substrate information}

%TODO maybe write this description
%TODO what's about global settings?

\section[Program application]{Program application}\label{application}


\subsection[Overview]{Overview}\label{application overview}

The example dataset distributed with the package \citep{vaas} comprises the results from running 114 GEN-III plates (AES Chemunex BLG 1030) in the PM mode of the OmniLog\textregistered \ reader.
The organisms used were two strains of \textit{Escherichia coli} (DSM 18039 $=$ K1 and the type strain DSM 30083\textsuperscript{T}) and two strains of \textit{Pseudomonas aeruginosa} (DSM 1707 and 429SC \citep{Selezska2012}).
The strains with a DSM number could be ordered from the Leibniz Institute DSMZ -- German Collection of Microorganisms and Cell Cultures.
\rcscom{
Johannes, has the fourth strain ...?
No, it has not, I will ask Elke Lang later if she has interest to put it into the collection.
}
Each strain was measured in two biological replicates, each comprising ten technical replicates, yielding a total of 80 plates.
To additionally investigate the impact of the growth age of cultures on the technical and biological reproducibility of the PM respiration kinetics, strain \textit{E. coli} DSM 18039 was grown on solid LB medium for nine different durations, from 16.75 h (t1) to 40.33 h (t9), respectively.
For each growth duration four technical replicates were performed except for t9 (which was repeated only twice), yielding 34 plates for this time-series experiment.
All further biological and experimental details of this dataset have been described previously \citep{vaas}.
The dataset \code{vaas\_et\_al} comes with the supporting package \pkg{opmdata} and can (if that package is installed, of course) be loaded using:
<<echo=FALSE>>=
options(prompt = "R> ")
options(continue = "   ")
library("methods") # necessary?
@

<<>>=
library("opm")
data("vaas_et_al", package = "opmdata")
@

The metadata included in these objects comprise seven entries.
The entry \textit{Experiment} denotes the biological replicate or the affiliation to the time-series experiments.
The keys \textit{Species} and \textit{Strain} refer to the organism used for the respective experiment (see above), and \textit{Slot} (either \textit{A} or \textit{B}) indicates whether the plate was placed in the left or the right half of the OmniLog\textregistered \ reader.
(Note that for an assessment of the reproducibility of the curves the slot is occasionally of relevance.)
Two additional entries contain the index of the time point and the corresponding sample point in minutes for the time series experiment.
The key \textit{Plate number} indicates the technical replicate (per biological replicate).
The combination of the keys \textit{Strains}, \textit{Species}, \textit{Experiment} and \textit{Plate number} results in a unique label which unequivocally annotates every single plate.

Use \code{?opmdata::vaas\_et\_al} to obtain further details.
The subsets \code{vaas\_1} and \code{vaas\_4} are described in the \pkg{opm} manual.
Use \code{?vaas\_1} and \code{?vaas\_4} to view their help pages.


\subsection[Data import]{Data import}\label{application import}

The following code describes the import of the OmniLog\textregistered \ CSV file(s) into the \pkg{opm} package.
The CSV files with the OmniLog\textregistered \ raw data should be stored in one to several user-defined folders.
Setting the working directory of \proglang{R} to the parent folder of these using \code{setwd()} frequently facilitates file selection, but in principle the user can provide any number of paths to input files and/or directories containing such files to the function \code{read\_opm()}, which can load several CSV files (and also \proglang{YAML} files generated by \pkg{opm}) at once.
A restriction of the input functions is that they can only read CSV files that only contain the measurements from a single plate per file (either a PM plate or a single Gen-III plate measured in either PM- or identification modus).
But the package contains a function \code{split_files()} which can be used to split CSV files with multiple plates into one file per plate
For details see the \pkg{opm} manual; all functions relevant here are contained in a family of functions called ``IO-functions'' with according cross-references.

To illustrate the file import step by step, a set of example input CSV files is provided with the package.
Before starting, please load the \pkg{opm} package by typing:
<<>>=
library("opm")
@
Then use the built-in function \code{opm\_files()} to find the example files in your \proglang{R} installation and check whether this returns a vector of nine file names:
<<results=hide>>=
(files <- opm_files("testdata"))
stopifnot(length(files) == 9) # => error if there were not nine files
@
(It might fail in very unusual \proglang{R} installation situations; in that case, the files must be found manually.)
One of these files contains multiple plates and acts as an example for \code{split_files()}; the other ones can be read directly.

From the given vector of file and/or directory names, files can be easily selected and deselected using globbing or regular-expression patterns.
For instance, for reading the three example files in ``new style'' CSV format (see Section~\ref{methods import}), use the following code.
After performing this step, the \proglang{OPMS} object should contain three plates, as indicated by the customized \code{summary()} function:

<<results=hide>>=
summary(example.opm <- read_opm(files, include = "*Example_?.csv.xz"))
stopifnot(length(example.opm) == 3) # => error if there were not three plates
@

As previously addressed, instead of a single file name the user could also provide several file names to \code{read\_opm()}, or a mixture of file and directory names; if these are contained as subdirectories of the current working directory, \code{read\_opm(".")} or \code{read\_opm(getwd())} would be sufficient to input these files.
To filter the files with patterns, the arguments \code{exclude} and \code{include} are available.
There is also a \textit{demo} mode allowing the user to check the effect of applying these arguments before actually reading files.
One can use the \code{gen.iii} argument to trigger the automated conversion of the plate type to, e.g., ``Gen III'' or ``ECO'' plates run in ``PM'' mode, or convert later on using the \code{gen\_iii()} function itself.
Plate-type conversions to one of the ``PM'' modes are disallowed (and are, to the best of our knowledge, not relevant in practice anyway).
The plate type is crucial, as it is disallowed to integrate distinct plate types into a single \proglang{OPMS} objects.
The reason is that comparing the same well positions from distinct plate types would be almost always equivalent to comparing apples and oranges.

If more than one plate of the same plate type is read, however, data from all files are automatically integrated in a single \proglang{OPMS} object.
To read plates from several types at once, have a look at the documentation of the \code{convert} argument in the manual.
If one uses \code{read\_opm(..., convert = "grp")}, a named list is created with, as each list element, one \proglang{OPM} or \proglang{OPMS} object per plate type, depending on whether only a single plate of that plate type, or several such plates, have been found.
The objects for each plate type encountered could then easily be accessed \textit{via} the names of the list.

A single plate could also be imported using, e.g.,:
<<>>=
example.single <- read_single_opm(files[1])
@

But this might only occasionally be useful, as \code{read\_opm} can cope with single files, too.


\subsection[Integration and manipulation of metadata]{Integration and manipulation of metadata}\label{application metadata}

Several ways for linking metadata to \proglang{OPM} or \proglang{OPMS} objects are possible.
The easiest one is probably the batch-inclusion after creating a template with plate identifiers associating it with metadata.
In the first step, either a data frame to be manipulated within \proglang{R} or a CSV file to be modified with a suitable editor are created.
The \pkg{opm} package supports metadata integration by creating a template for such a table from an \proglang{OPM} or \proglang{OPMS} objects that contains plate identifiers in the first columns; by default the keys \textit{Setup Time}, \textit{Position} and \textit{File}.
These data must not be changed, ensuring that the package can later on link the metadata to the dedicated plates according to these identifiers.

In the \pkg{opm} manual, most functions relevant for metadata manipulation are contained in a family called ``metadata-functions'' with according cross-references.
For the collection of a metadata template in a data frame to be manipulated in \proglang{R}, use this command:
<<width=9>>=
metadata.example <- collect_template(files, include = "*Example_?.csv.xz")
@

For the generation of a metadata template file, the following command can be used:
<<width=9>>=
collect_template(files, include = "*Example_?.csv.xz",
  outfile = "example_metadata.csv")
@
This will result in a file ``example\_metadata.csv'' in the current working directory (whose name is accessible using \code{getwd()}).
If other metadata have previously been collected, by default a pre-existing file with the same name will be reused.
The pre-defined columns will be respected, novel rows be added, old metadata will be kept and identifiers for novel files will be included and their so far empty metadata columns are set to missing data (NA).
You can also provide the location of another previously created metadata file with the \code{collect\_template()} argument \code{previous}.

The generated CSV file could then be edited using external software; for the purpose of this tutorial, we load it directly and manipulate it in \proglang{R}.
To avoid the usual changes in data format and header of the table during the import a customized import function was implemented as a wrapper for \code{read.delim()}:
<<width=10>>=
metadata.example <- to_metadata("example_metadata.csv")
@
Per default, this expects CSV columns separated by tabulators, with the fields protected by quotes.
To input other formats, consider the \code{sep} argument for defining an alternative column separator, as well as the \code{strip.white} argument for turning the removal of whitespace at the beginning and end of the fields on or off (which is relevant if a spreadsheet program exports CSV \textit{without} quotes).
Now the user could add information to the data frame by calling \code{edit()}, which would open the \proglang{R} editor, or by any other way of manipulating data frames in \proglang{R}.
New columns could be defined, or the existing metadata modified.
But the first columns must remain unchanged because they are needed to identify individual PM plates for linking them to their meta-information.
As an example, we here add an (arbitrary) \textit{Colour} column with the values ``blue'', ``red'' and ``yellow'':
<<width=10>>=
metadata.example[, "Colour"] <- c("blue", "red", "yellow")
@
Now the metadata are ready to be included into the previously generated \proglang{OPMS} object:
<<>>=
example.opm <- include_metadata(example.opm, md = metadata.example)
@
The metadata could then be received as follows:
<<results=hide>>=
metadata(example.opm)
@
This returns the entire metadata entries as a list.
By default only the added metadata are included in the object, but not the identifiers used for assigning data frame rows to plates.

One might want to tidy the files up if they are not needed any more:
<<width=10>>=
unlink("example_metadata.csv")
@

A couple of other functions have been implemented for manipulating metadata included in \proglang{OPM} and \proglang{OPMS} objects.
For instance, the entire meta-information, or specific entries, can be set using the replacement function \code{metadata()<-} (see the \pkg{opm} manual for details).
In the following we discuss metadata modification using \code{map\_metadata()}.

Making use of the exemplar generated above, the key \textit{Colour} could be changed to \textit{Colony colour} as follows:

<<results=hide>>=
(md.map <- metadata_chars(example.opm, values = FALSE))
@

This yields a character vector including itself as \code{names} attribute, thus implying an identity mapping.
Next the new labels will be defined and will then be exchanged with the old ones using \code{map\_metadata()}.

<<results=hide>>=
md.map["Colour"] <- "Colony colour"
example.opm <- map_metadata(example.opm, md.map, values = FALSE)
metadata(example.opm)
@

The keys should have been changed to \textit{Colony colour} now but the values should have remained unaffected.
In addition to mapping based on character vectors, a mapping function could also have been used.
By setting their argument \code{values} to \code{TRUE}, the functions \code{metadata\_chars()} and \code{map\_metadata()} could be used as well to modify values instead of key.
For instance, assume any entries ``red'' in the field denoted \textit{Colony colour} should be changed to ``green'':

<<results=hide, width=10>>=
(md.map <- metadata_chars(example.opm, values = TRUE))
md.map["red"] <- "green"
example.opm <- map_metadata(example.opm, md.map, values = TRUE)
metadata(example.opm)
@

This command will transform all entries in the table with the value "red" to "green".
Other values, as well as the keys, should be unaffected.
It is possible to map other types of entries such as numeric vectors by requesting their coercion to the character type; see the \pkg{opm} manual for details.

\subsection[Batch conversion of many files]{Batch conversion of many files}\label{application batch}

In addition to \code{read\_opm()} and \code{read\_single\_opm()}, which need to be called before an interactive exploration of PM data, batch-processing large numbers of files by converting them from CSV (or previously generated \proglang{YAML}) to \proglang{YAML} format, optionally after aggregating the raw data by estimating curve parameters and integrating metadata, is also possible.
Again there is a \textit{demo} mode to first investigate the attempted conversions:
<<results=hide, width=10>>=
batch_opm_to_yaml(files, include = "*Example_?.csv.xz",
  aggr.args = list(boot = 100, method = "opm-fast"),
  outdir = ".", demo = TRUE)
@
The arguments \code{aggr.args} and \code{md.args} control aggregation and metadata incorporation, respectively; details on both processes are given below, and for the exact use of these arguments see the \pkg{opm} manual.
The following command would thus read three of the seven example input files, estimate two of the four curve parameters using the fast native method including 100 rounds of bootstrapping, and store the resulting \proglang{YAML} files (one per plate) in the current working directory (given by ``.''):
<<results=hide, width=10>>=
batch.result <- batch_opm_to_yaml(files, include = "*Example_?.csv.xz",
  outdir = ".",
  aggr.args = list(boot = 100, method = "opm-fast"))
@
By default, progress messages are printed to the screen.
The return value, here assigned to the \code{batch.result} variable, also contains all information about the success of the individual file conversions.
The \code{run_opm.R} script distributed with the package is an \proglang{Rscript}-dependent command-line tool for non-interactively running such file conversions.
Its location in the file system can be obtained using

<<results=hide, width=10>>=
opm_files("scripts")
@


\subsection[Aggregating data by estimating curve parameters]{Aggregating data by estimating curve parameters}\label{application aggregating}

The package brings along an \proglang{OPMS} object, named \code{vaas\_et\_al}, containing multiple full 96-well plates, aggregated data (curve parameters), and metadata.
For demonstration purposes a subset of one plate, provided in the object \code{vaas\_1}, will be used:

<<>>=
data("vaas_1")
@

Data aggregation (curve-parameter estimation) can be performed using \code{do\_aggr()}.
In the \pkg{opm} manual, this one and the other functions relevant for data aggregation are contained in a family called ``aggregation-functions'' with according cross-references.
\code{vaas\_1} already contains aggregated data but we will re-calculate some for demonstration purposes.
For invoking the fast estimation method, use:

<<results=hide, width=10>>=
vaas_1.reaggr <- do_aggr(vaas_1, boot = 100, method = "opm-fast")
@

This will only estimate two of the four parameters, namely A and AUC.
(Screen messages output by \code{boot.ci()} might be annoying but can usually be ignored.)
Information about the data aggregation settings is available \textit{via}:

<<results=hide>>=
aggr_settings(vaas_1)
aggr_settings(vaas_1.reaggr)
@

and the aggregated data can be extracted as a matrix \textit{via}:

<<width=10>>=
vaas_1.aggr <- aggregated(vaas_1)
vaas_1.reaggr.aggr <- aggregated(vaas_1.reaggr)
@

The default function of \code{do\_aggr()} includes 100-fold bootstrapping of the data to obtain confidence intervals.
As this is a time-consuming intensive process (if \pkg{grofit} is used), it may be split over several cores on a multicore machine if the \pkg{multicore} \proglang{R} package is available by setting the cores argument to a value larger than one.

One can also specify different spline fitting methods using \code{method = "spline"}.
  Options such as the spline type, the number of knots used for the spline and
  other options for the splines can be easily set using the function \code{set\_spline\_options} (which can only be used in conjunction with \code{method = "spline"}).
  To essentially reproduce the results from \code{method = "grofit"} we use smoothing splines (and for the sake of computing time in this vignette we only use 10 bootstrap replicates to compute confidence intervals for the parameter estimates):
<<smoothing_splines,results=hide>>=
op <- set_spline_options(type = "smooth.spline")
vaas_1.aggr2 <- do_aggr(vaas_1, boot = 10, method = "spline", options = op)
@
Other spline types can be specified \textit{via} the \code{type} argument in the function \code{set\_spline\_options}.

\subsection[Manipulation of OPM and OPMS data]{Manipulation of \proglang{OPM} and \proglang{OPMS} data}\label{application manipulation}

In the \pkg{opm} manual, the functions relevant for retrieving information contained in \proglang{OPM} or \proglang{OPMS} objects are included in a family called ``getter-functions'' with according cross-references.

For instance, the user may wish to select specific wells from the input plates, which are present in a 96-well layout, numbered from A01 to H12.
The function \code{dim()} provides the dimensions of an \proglang{OPMS} object as a three-element vector comprising (i) number of contained \proglang{OPM} or \proglang{OPMA} objects, (ii) the number of time points (of the first contained plate; these values need not be uniform within an \proglang{OPMS} object), and (iii) the number of wells (which must be uniform within an \proglang{OPMS} object).

To extract, for example, only the data from wells G11 and H11 together with the negative-control well A01 from the dataset \code{vaas\_et\_al} the bracket operator defined for the \proglang{OPMS} class has to be invoked as follows:

<<results=hide, width=10>>=
data("vaas_et_al", package = "opmdata")
vaas.small <- vaas_et_al[, , c("A01", "G11", "H11")]
dim(vaas.small)
@

\proglang{R} users should be familiar with this subsetting style, which was modelled after the style for multidimensional arrays, even though the internal representation is quite different.

After metadata have been added, \proglang{OPM} and \proglang{OPMS} objects can be queried for their content.
Specialized infix operators \code{\%k\%} and \code{\%q\%} (for \code{\%K\%} and \code{\%Q\%} see the \pkg{opm} manual) have been modelled in analogy to \proglang{R}'s \code{\%in\%} operator.
The user may be interested whether an \proglang{OPM} or \proglang{OPMS} object contains a specific value associated with a specific metadata key, or the key associated with any value, or combinations of keys and/or values.
\code{\%k\%} allows the user to search in the metadata keys.
The user can test whether all given keys are present as names of the metadata.
\code{\%q\%} tests whether all given query keys are present as names of the metadata and refer to the same query elements.

Some examples using \code{vaas\_et\_al} are given in the following.
This \proglang{OPMS} object contains a metadata key \textit{Experiment} with the three possible values \textit{Time series}, \textit{First replicate}, and \textit{Second replicate}, and a metadata key \textit{Species} with  either \textit{Escherichia coli} or \textit{Pseudomonas aeruginosa} as values.

<<>>=
data("vaas_et_al", package = "opmdata")
@

Which plates within \code{vaas\_et\_al} have \textit{Experiment} as metadata key?

<<results=hide>>=
"Experiment" %k% vaas_et_al
@

Which plates within \code{vaas\_et\_al} have \textit{Experiment} and \textit{Species} as metadata key?

<<results=hide>>=
c("Experiment", "Species") %k% vaas_et_al
@

Which plates within \code{vaas\_et\_al} have \textit{Experiment} and \textit{Species} as metadata key with the respective values \textit{First replicate} and \textit{Escherichia coli}?

<<results=hide, width=8>>=
c(Experiment = "First replicate",
  Species = "Escherichia coli") %q% vaas_et_al
@

Which plates within \code{vaas\_et\_al} have \textit{Species} as metadata key associated with the value \textit{Escherichia coli} or the value \textit{Bacillus subtilis}?

<<results=hide, width=10>>=
list(Species = c("Escherichia coli", "Bacillus subtilis")) %q% vaas_et_al
@

In addition to conducting queries with alternatives, using lists as queries would also allow for nested queries (as the metadata entries could also be nested).
The results of these infix operators are reported as logical vector with one value per plate; the usual \proglang{R} functions such as \code{all()}, \code{any()} or \code{which()} could be applied to work on these vectors.
They could also be used directly as the first argument of the bracket operator for \proglang{OPMS} objects to create subsets:

<<width=8>>=
vaas.e.coli.1 <- vaas_et_al[c(Experiment = "First replicate",
  Species = "Escherichia coli") %q% vaas_et_al]
@

Alternatively, the user may wish to subset a certain part of the data set using the function \code{subset()}, which is based on these kinds of querying for metadata keys and their values.
Prior to this, the user could check the keys of the metadata:

<<results=hide>>=
data("vaas_et_al", package = "opmdata")
metadata_chars(vaas_et_al, values = FALSE)
@

The values in the metadata could be obtained by using \code{values = TRUE}.
Additionally, the user can check the values of special keys in the metadata:

<<results=hide>>=
metadata(vaas_et_al, "Species")
@

The resulting vectors could then also be used for mapping old metadata keys or values to novel ones (for details see Section~\ref{methods metadata}).

The presented plotting results of \code{xy\_plot()} and \code{level\_plot()} (see Section~\ref{application plotting}) show selected subsets of \code{vaas\_et\_al}.
In our example below, the function \code{subset()} extracts the plates which contain the value \textit{First replicate} in the metadata key \textit{Experiment} and the value \textit{6} in the key \textit{Plate number}, resulting in one representative technical repetition and thus four plates (because four strains were involved) from the data set \code{vaas\_et\_al}:

<<width=8>>=
vaas.1.6 <- subset(vaas_et_al,
  query = list(Experiment = "First replicate", 'Plate number' = 6))
@

Providing the desired combination of metadata keys and values as a list offers a maximum of flexibility, but other approaches are also implemented, as well as the selection of plates based on the presence of keys only (like \code{\%k\%} described above; it makes not much sense for \code{vaas\_et\_al} whose plates are uniform regarding the keys), and nested queries (like \code{\%q\%} with a list described above; makes of course more sense if the metadata contain nested entries).
The \code{subset()} function also has a ``time'' argument that allows one to create a subset containing only the time points that were common to all plates.
This is useful because deviations regarding the overall measurement hours might exist.
See the manual for details.

In addition to plate-wise querying and subsetting of \proglang{OPMS} objects, a number of conversion functions for selected content of all plates have been implemented.
The \pkg{opm} manual lists them in a family of functions called ``conversion-functions'' with according cross-references.
For instance, the user may wish to explore the aggregated curve parameters (lag phase $\lambda$, steepness of the slope $\mu$, maximum curve height A, and area under the curve AUC).
These may be exported either as matrix or data frame using \code{extract()}:

<<width=9>>=
vaas.mu <- extract(vaas_et_al, dataframe = TRUE,
  as.labels = NULL, subset = "mu")
@

To extract also the full or partial set of metadata, it is sufficient to add a list of desired metadata:

<<width=8>>=
vaas.mu <- extract(vaas_et_al, dataframe = TRUE,
  as.labels = list("Experiment","Number of sample time point",
    "Plate number", "Slot", "Species", "Strain", "Time point in min"),
  subset = "mu")
@

This only works if this meta-information is present for the plates under study.
Once a data frame is exported, these metadata will be contained in additional columns; once a matrix is exported, they will be used to construct the row names.

\subsection[Plotting functions]{Plotting functions}\label{application plotting}
\subsubsection[Plotting functions for raw data]{Plotting functions for raw data}

In the \pkg{opm} manual, the functions relevant for plotting are contained in a family called, well, ``plotting-functions'' with according cross-references.
The function \code{xy\_plot()} displays the respiration curves as such (see Figure~\ref{fig:xy_plot}).
In our example the selected \proglang{OPMS} object \code{vaas.1.6} is the subset of the dataset \code{vaas\_et\_al} constructed in Section~\ref{application manipulation}:

<<fig=FALSE, width=8>>=
xy_plot(vaas.1.6, main = "E. coli vs. P. aeruginosa",
  include = list("Species", "Strain"))
@

\begin{figure}

\scalebox{1.225}{

<<fig=TRUE, echo=FALSE, width=18, height=12>>=
print(xy_plot(vaas.1.6, main = "E. coli vs. P. aeruginosa",
  include = list("Species",   "Strain")))
@

}

\caption{
\label{fig:xy_plot}
PM curves from the 6th technical repetition of the first biological repetition plotted using \code{xy\_plot()} and by default arranged according to the factual plate layout.
The respective curves from all four strains are superimposed; the affiliation to each strain is indicated by colour (see the legend).
The x-axes show the measurement time in hours, the y-axes the measured colour intensities in OmniLog\textregistered \ units.
}

\end{figure}

Using the argument \code{main}, the user can include a main title in the plot; if it is omitted, by default the title is automatically constructed from the plate type.
Likewise, the well coordinates are automatically converted to substrate names (details can be set using additional arguments).
The content of the legend (mainly a description of the assignment of the colours to the curves) is also determined automatically.
The argument \code{include} refers to the metadata and allows the user to choose which entries should be used for assigning curve colours and accordingly be included in the legend.
In the example the combination of species and strain is used, yielding four distinct colours.
If \code{include} is not used, the colours are assigned per plate.

The plotting of sub-panels (see Figure~\ref{fig:xy_plot_2}) works in the same way; the only difference is the previous manipulation of the dataset:

<<fig=FALSE, width=8>>=
xy_plot(vaas.1.6[, , c("A01", "G11", "H11")],
  main = "E. coli vs. P. aeruginosa", include = list("Species", "Strain"))
@

\begin{figure}

\scalebox{1.225}{

<<fig=TRUE, echo=FALSE, width=9, height=6>>=
print(xy_plot(vaas.1.6[, , c("A01", "G11", "H11")],
  main = "E. coli vs. P. aeruginosa",
  include = list("Species", "Strain")))
@

}

\caption{
\label{fig:xy_plot_2}
Selected PM curves from the 6th technical repetition from the first biological repetition plotted using \code{xy\_plot()}.
The respective curves from all four strains are superimposed, the affiliation to each strain indicated by colour (see the legend).
The x-axes show the measurement time in hours, the y-axes the measured colour-value units.
}

\end{figure}

The function \code{level\_plot()} (see Figure~\ref{fig:level_plot}) provides false-colour level plots from the raw respiration measurements over time:

<<results=hide, fig=FALSE, width=8>>=
level_plot(vaas.1.6, main = "E. coli vs. P. aeruginosa",
  include = list("Species", "Strain"))
@

\begin{figure}

\scalebox{1.225}{

<<fig=TRUE, echo=FALSE, width=18, height=12>>=
print(level_plot(vaas.1.6, main = "E. coli vs. P. aeruginosa",
  include = list("Species", "Strain")))
@

}

\caption{
\label{fig:level_plot}
Visualization of PM curves using the function \code{level\_plot()}.
Each respiration curve is displayed as a thin horizontal line, in which the curve height as measured in colour-value units is represented by color intensity (darker parts indicate higher curves).
The x-axes correspond to the measurement time in hours.
}

\end{figure}

Again, a main title can be set explicitly.
Furthermore, the argument \code{include} again refers to the metadata and allows the user to choose the information to be included in the header for annotating the plates.
In the example the combination of species and strain is used.

\subsubsection[Plotting the aggregated data]{Plotting the aggregated data}\label{application plotting-aggregated}

The function \code{heat\_map()} (see Figure~\ref{fig:heat_map}) provides false-colour level plots in which both axes are rearranged according to clustering results.
In the context of PM data, it makes most sense to apply it to the estimated curve parameters.
This \pkg{opm} function is a wrapper for \code{heatmap()} from the \pkg{stats} and \code{heatmap.2()} from the \pkg{gplots} package with some adaptations to PM data.
For instance, row groups can be automatically constructed from the metadata.
The function must be applied to matrices or data frames constructed using \code{extract()}:

<<results=hide, fig=FALSE, width=10>>=
vaas.1.6.A <- extract(vaas.1.6, as.labels = list("Species", "Strain"),
  dataframe = TRUE)
vaas.1.6.A.hm <- heat_map(vaas.1.6.A, as.labels = "Strain",
  as.groups = "Species")
@

\begin{figure}
\scalebox{1.225}{
<<fig=TRUE, echo=FALSE, results=hide, width=9, height=6>>=
print(heat_map(vaas.1.6.A, as.labels = "Strain", as.groups = "Species"))
@
}
\caption{
\label{fig:heat_map}
Visualization of the clustered results from the curve parameter maximum height (A) for each substrate using the function \code{heat\_map()}.
The x-axis corresponds to the substrates clustered according to the similarity of their values over all plates; the y-axis corresponds to the plates clustered to the similarity of their values over all substrates.
As row labels, the strain names were selected, whereas the species affiliations was used to assign row group colours (bars at the left side).
The central rectangle is a substrate $x$ plate matrix in which the colours represent the classes of values.
The default colour setting uses topological colours, with deep violet and blue indicating the lowest values and light brown indicating the highest values.
}
\end{figure}



\rcscom{
Lea, please provide examples for the use of \code{ci\_plot()} in conjunction with \code{extract()}.
You can use the examples from the manual here because they are unsuitable for the manual.
Afterwards, include suitable examples in the manual.
}
<<results=hide, fig=FALSE, width=10>>=
(x <- extract(vaas_et_al, as.labels = list("Species", "Strain"),
  dataframe = TRUE))
# without normalisation
(y <- extract(x, as.groups = TRUE,  norm.per = c("none")))
ci_plot(y[,1:8])
# normalisation by plate-means
(y <- extract(x, as.groups = TRUE,  norm.per = c("row")))
ci_plot(y[,1:8])
# normalisation by well-means
(y <- extract(x, as.groups = TRUE,  norm.per = c("column")))
ci_plot(y[,1:8])
# normalisation by subtraction of well-mean of A10
(y <- extract(x, as.groups = TRUE,  norm.per = c("column"), norm.by = c(10),
  subtract = TRUE))
ci_plot(y[,1:15])
@
The \code{ci\_plot()} function (see Figure~\ref{fig:ci_plot}) provides a visualization of the point estimator and its 95\% confidence interval calculated \textit{via} bootstrapping during aggregation of curves into parameters.
The user is free to select the subsets of interest \textit{via} the bracket operator as described above (see Section~\ref{application manipulation}):

<<results=hide, fig=FALSE, width=10>>=
ci_plot.legend <- ci_plot(vaas.1.6[, , 1:3],
  as.labels = list("Species", "Strain"), subset = "A",
  legend.field = NULL, x = 150, y = 3)
@

\begin{figure}
\scalebox{1.225}{
<<fig=TRUE, echo=FALSE, results=hide, width=9, height=6>>=
print(ci_plot(vaas.1.6[, , 1:3], as.labels = list("Species", "Strain"),
  subset = "A", legend.field = NULL, x = 150, y = 3))
@
}
\caption{
\label{fig:ci_plot}
Comparison of point estimates and their 95\% confidence intervals for the parameter maximum height (A) observed from four strains, using \code{ci\_plot()}.
Shown are the results on the three wells A01 (negative control), A02 (Dextrin) and A03 (D-Maltose) as indicated by the sub-plot titles.
}
\end{figure}


\subsection[Statistical comparisons of group means]{Statistical comparisons of group means}\label{application group-means}

\rcscom{
Vorschlag Johannes: Wir sollten hier ein plakatives Beispiel nehmen, in dem wir sowohl Gruppenvergleiche haben, die nicht verschieden voneinander sind, wie auch Gruppenvergleiche, die signifikant verschieden voneinander sind und bei denen man auch einen Unterschied in der effect size sehen kann. Daher sollten wir auf den \code{vaas\_et\_al} Datensatz zurueckgreifen, wie unten in blauer Farbe beschrieben.
Die beiden Beispiel-Figures von Lea (momentan Fig. 12 und 13) halte ich nicht fuer so ideal, denn Fig. 12 beinhaltet nur einen paarweisen Vergleich, kein multiple comparison, und in Fig. 13 gibt es keinen Unterschied bei irgendeinem der paarweisen Vergleich - es wird dem Leser das Potential einer solchen Abbildung nicht vollstaendig klar.Ich habe mir trotzdem erlaubt, bei diesen und einigen anderen Figures etwas die Groesse anzupassen.
}

To demonstrate the functionality of \code{opm\_mcp()}, a comparison of four different bacteria, with each ten replicates using GEN-III microplates, will be utilized.
Do these four bacteria differ in, e.g., the mean value of, e.g., curve parameter A of, e.g, well A1?
Here, a statistical comparison of four groups (four organisms) of each 10 values (curve parameter A of 10 replicates of well A1) will be performed.
First, from the data set \code{vaas\_et\_al}, a subset of the ten technical replicates of GEN-III plates from the first biological replicate and also for the curves from well G06 only will be made.
<<results=hide, width=8>>=
vaas.subset.mcp <- subset(vaas_et_al[, , "G06"], 
  list(Experiment = "First replicate"))
@

For each of the four strains, ten curves are shown (see Figure~\ref{fig:xy_plot_mcp}). 

<<fig=FALSE, width=10>>=
xy_plot(vaas.subset.mcp, include = ~ Strain, neg.ctrl = FALSE,
  legend.fmt = list(space = "right"))
@

\begin{figure}

\scalebox{1.225}{

<<fig=TRUE, echo=FALSE, results=hide, width=9, height=6>>=
xy_plot(vaas.subset.mcp, include = ~ Strain, neg.ctrl = FALSE, 
  legend.fmt  = list(space = "right"))
@

}

\caption{
\label{fig:xy_plot_mcp}
PM curves from the ten technical replicates of the first biological repetition plotted using \code{xy\_plot()}.
The respective curves from all four strains are superimposed; the affiliation to each strain is indicated by colour (see the legend).
The x-axes show the measurement time in hours, the y-axes the measured colour intensities in OmniLog\textregistered \ units.
}

\end{figure}


The statistical question to be addressed is the following.
Is there a significant difference between any of the four strains, based on the mean of the curve parameter A values as determined for each of the ten curves?
Here, a multiple comparison of group means is performed using \code{opm\_mcp()}.
The groups to be compared are defined by the argument ``model''. The argument ``m.type'' specifies the model types to use in model fitting, e.g., a linear model (lm), a generalized linar model (glm), or an analysis of variance (aov). The argument ``mcp.def'' basically defines the type of contrast matrix to be used for the multiple comparison. In principle, the contrast matrix defines what type of comparison should be made. Concrete, among the groups defined by the argument ``model'', which of these should actually be included in pariwise comparisons? A detailed explanation of contrast matrices is shown further below. \rcscom{Lea, is this the correct idea?}

<<results=hide, width=8>>=
(x <- opm_mcp(vaas.subset.mcp, model = ~ Strain, m.type = "aov",
  mcp.def = mcp(Strain = "Tukey")))
@

The results shown below indicate that all group means are significantly different from each other, except for the comparison of strains DSM30083T and DSM1707:

<<results=hide, width=8>>=
summary(x)
@

\rcscom{
Is it possible to show here at least part of the output from \code{summary(x)}? We surely do not need the long list at the beginning, but would it be possible to show the output containing the estimates, the p-values, etc?
}

The results of the statistical test can be plotted (see Figure~\ref{fig:ci_plot_mcp}). 
For example, the mean of curve parameter A values of strain DSM 30083T is 26.615 units larger than the mean of A values of strain DSM18039 (see the estimate results above, provided by \code{summary(x)}). 
Therefore, point estimator of difference between these two strain is plotted at position 26.615 on the x-axis. 
In pairwise comparisons where the 95\% confidence interval crosses the vertical dashed line at zero there is no significant difference between the group means. 
The more distant the 95\% confidence interval is from the dashed zero line, the larger the biological effect of the difference, irrespective of the size of the p-value. 
For example, the results from the statistical calculations indicate that all significant differences are highly significant (p < 0.001). 
However, the position of the 95\% confidence interval indicates that the difference between strains DSM18039 and 429SC1 (i.e, the effect size) is much larger than between strains DSM30083T and 429SC1.


<<fig=FALSE, width=8>>=
op <- par(no.readonly = TRUE) # default plotting settings
par(mar = c(3, 15, 3, 2))
plot(x)
par(op)
@

\begin{figure}

\scalebox{1.225}{

<<fig=TRUE, echo=FALSE, width=8, height=3>>=
op <- par(no.readonly = TRUE) # default plotting settings
par(mar = c(3, 15, 3, 2))
plot(x)
par(op)
@

}

\caption{
\label{fig:ci_plot_mcp}
95\% confidence interval for comparison of group means between the strains. All pairwise comparisons are shown. The filled black circle indicates the point estimator of difference betweeen the mean of groups. 
}

\end{figure}

\rcscom{
Kommentar Johannes: Bis hierhin hatten wir ein sehr simples und straight-forward example.
Nun koennte Lea, wie schon von ihr angedeutet, die Spezialitaeten erlaeutern, z.B. was eine contrast matrix ist, ect.
Anschliessend koennten wir nochmal ein echt komplexes Beispiel bringen, in dem man die volle Wucht der moeglichen Argumente in  \code{opm\_mcp()} ausnutzt, so dass der Leser nocheinmal wirklich vor Augen gefuehrt bekommt, was man mit  \code{opm\_mcp()} so alles Feines machen kann.
}

\rcscom{TODO Lea: here the application of \code{opm\_mcp()} with example and output-graphics.
reshaping the data}
<<results=hide, width=8>>=
data(vaas_4)
x <- opm_mcp(vaas_4, model = ~ Species + Strain,
  do.mcp = FALSE)
@

\rcscom{Explain contrasts, refer to \code{contrMat() from \pkg{multcomp}}}
<<results=hide, echo=FALSE, width=8>>=
n <- c(10,20,30,40)
 names(n) <- paste("group", 1:4, sep="")
 contrMat(n)  # Dunnett is default
 contrMat(n, base = 2)	# use second level as baseline
 contrMat(n, type = "Tukey")
 contrMat(n, type = "Sequen")
 contrMat(n, type = "AVE")
 contrMat(n, type = "Changepoint")
 contrMat(n, type = "Williams")
 contrMat(n, type = "Marcus")
 contrMat(n, type = "McDermott")
@

\rcscom{comparison according metadata 'Species'}
<<results=hide, width=8>>=
x <- opm_mcp(vaas_4, model = ~ Species, m.type = "lm",
  mcp.def = mcp(Species = "Dunnett"))
@

\rcscom{Explain Confidence intervals for differences of group means, point out the difference to the 95\%CI of the curve-parameter estimators.
Figure:
informative plotting of the CI:}
<<results=hide, width=8>>=
op <- par(no.readonly = TRUE) # default plotting settings
par(mar = c(3, 20, 3, 2))
plot(x)
par(op)
@

\begin{figure}
\scalebox{0.9}{
<<fig=TRUE, echo=FALSE, results=hide, width=9, height=3>>=
op <- par(no.readonly = TRUE) # default plotting settings
par(mar = c(3, 20, 3, 2))
plot(x)
par(op)
@
}
\caption{
\label{fig:opm_mcp_CI_plotI}
95\% confidence interval for comparison of group means between the species.
}
\end{figure}

\rcscom{more sophisticated with maually defined set of contrast:}

<<results=hide, width=8>>=
 contr <- rbind(
   "A01 (Negative Control) - A02 (Dextrin)" = c(1, -1, 0, 0),
   "A01 (Negative Control) - A03 (D-Maltose)" = c(-1, 0, 1, 0),
  "A01 (Negative Control) - A04 (D-Trehalose)" = c(-1, 0, 0, -1),
   "A03 (D-Maltose) - A04 (D-Trehalose)" = c(0, 0, 1, -1))
 x <- opm_mcp(vaas_4[, , 1:4],
    model = ~ Well, m.type = "lm", mcp.def = contr)
@


\begin{figure}
\scalebox{0.9}{
<<fig=TRUE, echo=FALSE, results=hide, width=9, height=3>>=
 # creating an informative plot
 op <- par(no.readonly = TRUE) # default plotting settings
 par(mar = c(3, 20, 3, 2))
 plot(x)
 par(op) # reset plotting settings
@
}
\caption{
\label{fig:opm_mcp_CI_plotII}
95\% confidence interval for manyally chosen comparison of group means on a specifically selected set of wells.
}
\end{figure}


\subsection[Discretization and phylogenetic data export]{Discretization and phylogenetic data export}\label{application discrete-phylogeny}

After suitable subsetting and extraction of one of the curve parameters, data can be discretized and optionally also be exported for analysis with external phylogeny software.
In the \pkg{opm} manual, the functions relevant for either task are contained in the families ``discretization-functions'' and ``phylogeny-functions'' with according cross-references.
Restricting the \code{vaas\_et\_al} example dataset to the two biological replicates yields an orthogonal dataset with 2$\times$10 replicates for each of the four strains for which we can calculate discretized parameters:

<<results=hide, width=8>>=
vaas.repl <- subset(vaas_et_al,
  query = list(Experiment = c("First replicate", "Second replicate")))
vaas.repl <- do_disc(vaas.repl)
@

Note that the resulting objects is an \proglang{OPMS} object with \proglang{OPMD} objects as elements.
Such objects contain discretized values available \textit{via} \code{discretized()} and the discretization settings used, which can be obtained using \code{disc\_settings()}.
This works much like \code{aggregated()} and \code{aggr\_settings()} explained above.
\code{disc\_settings()} also yields the computed discretization cutoffs.
The \code{subset()} function has a \code{positive} argument that allows one to create a subset containing only the wells that were positive in at least one plate or in all plates, as well as a \code{negative} argument.
See the manual for details.

\subsubsection[Discretization and export of text]{Discretization and export of text}\label{application discrete-text}

The \code{listing()} methods of the \proglang{OPMD} and \proglang{OPMD} classes create textual descriptions of the discretization results suitable for the direct inclusion in scientific manuscripts.

<<results=hide, width=8>>=
listing(vaas.repl, as.groups = NULL)
listing(vaas.repl, as.groups = list("Species"))
@

As usual, the results can be grouped according to specified metadata entries using the ``as.groups'' argument.
If this yields ambiguities (such as a negative reaction of the same well on one plate and a positive reaction on another plate), the result is accordingly renamed.
The ``cutoff'' argument can be used to define filters, keeping only those values that occur in a specified minimum proportion of wells.
See the manual for details.

The \code{listing()} function returns a character vector or matrix with the \proglang{S3} class ``OPMD\_listing'' or ``OPMS\_listing'', allowing for a special \code{phylo\_data()} function that further formats these objects. Accordingly, the following code snippets

<<results=hide, width=8>>=
phylo_data(listing(vaas.repl, as.groups = NULL))
phylo_data(listing(vaas.repl, as.groups = list("Species")))
@

would yield character scalars better suitable for exporting into text files using \code{write}. It is also possible to generate \proglang{HTML} output, yielding formatted text. Try

<<results=hide, width=8>>=
phylo_data(listing(vaas.repl, as.groups = NULL, html = TRUE))
phylo_data(listing(vaas.repl, as.groups = list("Species"), html = TRUE))
@

and note that the \code{phylo\_data()} function has a ``html.args'' argument.
Textual \proglang{HTML} output supports most of the formatting instructions  for the output of \proglang{HTML} tables described below (see \ref{application discrete-table}).
Note particularly how formatting \textit{via} a \proglang{CSS} file works.

The default settings of \code{do\_disc()} imply exact k-means partitioning into three groups (``negative'', ``ambiguous'' and ``positive''), treating all contained plates together.
Let $A_1$ and $A_2$ be the maximum-height parameters from two curves $C_1$ and $C_2$, respectively, and let us assume that $A_1 \geq A_2$ holds.
The algorithm then guarantees that if $C_2$ is judged as positive reaction then $C_1$ is also judged as positive; if $C_2$ is weak then $C_1$ is not negative; if $C_1$ is negative then $C_2$ is negative; and if $C_1$ is weak then $C_2$ is not positive.
There are not many other things the algorithm guarantees.
Note particularly that always three clusters result by default (one can omit the middle cluster, i.e. the ``weak'' reactions), irrespective of the input data.
That is, additionally checking the curve heights and particularly the ``cutoffs'' entry obtained \textit{via} \code{disc\_settings()} should be mandatory.

The manual describes the other discretization approaches available in \pkg{opm}, such as using \code{best\_cutoff()} instead of k-means partitioning, and using subsets of the plates, specified using stored metainformation.

\subsubsection[Discretization and export of tables]{Discretization and export of tables}\label{application discrete-table}

The \proglang{HTML} created by \pkg{opm} deliberately contains no formatting instructions.
Rather, it is possible (and recommended) to link it to a \proglang{CSS} file.
As the generated \proglang{HTML} is richly annotated with ``class'' attributes, which not only provide information on the structure of the file but also on the depicted data, very specific formatting can be obtained just by modifying one to several associated \proglang{CSS} files.

For the following example, we set the default \proglang{CSS} file to be linked from the generated \proglang{HTML} to the file that comes with \pkg{opm}.

<<results=hide, width=8>>=
opm_opt(css.file = grep("[.]css$", opm_files("auxiliary"), value = TRUE))
@

One could now easily create an \proglang{HTML} table from the discretized data and write it to a file:

<<results=hide, width=8>>=
vaas.html <- phylo_data(vaas.repl, format = "html",
  as.labels = list("Species", "Strain"), outfile = "vaas.html")
@

A practical problem is that the resulting \proglang{HTML} file is linked to its \proglang{CSS} file with a fixed path.
The formatting would thus get lost once the \proglang{HTML} file was copied to another system, without a warning.
So users might want to copy the predefined \proglang{CSS} file to the working directory and set it as default:

<<results=hide, width=8>>=
file.copy(grep("[.]css$", opm_files("auxiliary"), value = TRUE),
  "opm_styles.css", overwrite = TRUE)
opm_opt(css.file = "opm_styles.css")
@

The generated \proglang{HTML} would subsequently be linked to this file, and the two files could be distributed together.
The same mechanism works for text generation using \code{listing()} (see \ref{application discrete-text}).

Users who want to define their own \proglang{CSS} files can start with modifying the file shipped with \pkg{opm}.
Microsoft Windows users should consider that the path to the file must be provided in \proglang{UNIX} style, as obtained, e.g., using \code{normalizePath(x, winslash = "/")} if \code{x} is the path to the file.
This is according to World Wide Web standards and not determined by \pkg{opm}.

By default columns with measurement repetitions as specified using \code{as.labels} are joined together.
The \code{delete} argument specifies how to reduce the table: either not at all or keeping only the variable, parsimony-informative or non-ambiguous characters.
The legend of the table is as used in taxonomic journals such as IJSEM but could also be adapted.
Users can modify the headline, add sections before the table legend, or before or after the table.
The title and the ``meta'' entries of the resulting \proglang{HTML} can also be modified.
The \code{phylo\_data()} methods have an auxiliary function, \code{html_args}, which assists in putting together the arguments that determine the shape and content of the \proglang{HTML} output.
See the manual for further details.

\subsubsection[Fine-tuning the discretization]{Fine-tuning the discretization}\label{application discrete-finetuning}

One can also conduct discretization step-by-step by using the functions \code{best\_cutoff()} or \code{discrete()} after extracting matrices from the \proglang{OPMS} object.
This offers more flexibility (such as additional discretization approaches, e.g. the creation of multiple-state characters) but is also more tedious.

<<results=hide, width=8>>=
vaas.repl <- subset(vaas_et_al,
  query = list(Experiment = c("First replicate", "Second replicate")))
vaas.repl <- extract(vaas.repl,
  as.labels = list("Species", "Strain", "Experiment", "Plate number"))
@

%(As above, to specify \code{opm::} is not normally necessary but clashes with a \code{select} method defined in the \pkg{MASS} package sometimes need to be avoided.)
The A parameter can be discretized into (per default) 32 states using the theoretical range of 0 to 400 OmniLog\textregistered \ units (see Section~\ref{methods discrete-multistate}):

<<width=12>>=
vaas.repl.disc <- discrete(vaas.repl, range = c(0, 400))
@

This yields (at most) 32 distinct character states corresponding to the 32 equal-width intervals within 0 and 400.
Exporting the data in \textit{extended \proglang{PHYLIP} format} readable by \proglang{RAxML} \citep{stamatakis} would work as follows:

<<width=10>>=
phylo_data(vaas.repl.disc, outfile = "example_replicates.epf")
@

The other supported formats are \proglang{PHYLIP}, \proglang{NEXUS} and \proglang{TNT} \citep{goloboff}.
For discretizing the data not in equally spaced intervals but into binary characters including missing data, or ternary characters with a third, intermediary state between "negative" and "positive" the gap mode of \code{discrete()} can be used:

<<width=12>>=
vaas.repl.disc <- discrete(vaas.repl, range = c(120.2, 236.6), gap = TRUE)
@

Here the range argument provides not the overall boundaries of the data as before (at least as large as the real range), but the boundaries of a zone within the real range of the data corresponding to an area of ambiguous affiliation.
That is, values below 120.2 are coded as ``0'', those above 236.6 as ``1'', and those in between as ``?''.
The values used above were determined by k-means partitioning of the A values from the \code{vaas\_et\_al} dataset \citep{vaas}; there is currently no conclusive evidence that they can generally be applied.
The last command would result in the treatment of values within the given range as ``missing data'' (\code{NA} in \proglang{R}, ``?'' if exported).
To treat them as a third, intermediary character state, set \code{middle.na} to \code{FALSE}:

<<width=10>>=
vaas.repl.disc <- discrete(vaas.repl, range = c(120.2, 236.6),
  gap = TRUE, middle.na = FALSE)
@

The three resulting states, coded as ``0'', ``1'' and ``2'' (in contrast to ``0'', ``?'' and ``1'' above) would have to be interpreted as ``negative'', ``weak'' and ``positive''.
Exporting the data in one of the supported phylogeny formats would work as described above.
If the \code{do\_disc()} function described above calls \code{discrete()}, then only in gap mode and with \code{middle.na} set to \code{TRUE}, yielding a vector or logical matrix.


\subsection[Accessing substrate information]{Accessing substrate information}\label{application substrates}

The \pkg{opm} package contains a number of functions suitable for accessing precomputed information on the substrates of wells and plates.
In the manual, these functions are contained in the family ``naming-functions'' with according cross-references.
One usually would start a search by determining the exact spelling of an internally used name with \code{find\_substrate()}:

<<results=hide>>=
(names <- find_substrate(c("Glutamine", "Glutamic acid")))
@

The results is a list containing character vectors with the results for each query name as values.
Surprisingly, nothing was found for ``Glutamic acid'' but several values for ``Glutamine''.
The default \code{search} argument is ``exact'', which is exact (case-sensitive) matching  of \textit{substrings} of the names.
One might want to use ``glob'' searching mode:

<<results=hide>>=
(names <- find_substrate(c("L-Glutamine", "L-Glutamic acid"), "glob"))
@

But with so-called wildcards, i.e. ``*'' for zero to many and ``?'' for a single arbitrary character the search is more flexible:

<<results=hide>>=
(names <- find_substrate(c("*L-Glutamine", "*L-Glutamic acid"), "glob"))
@

This fetches all terms that end in either query character string, and does so case-insensitively.
Advanced users can apply the much more powerful ``regex'' and ``approx'' search modes; see the manual for details.

Once the internally used names have been found, information on the substrates can be queried such as their occurrences and positions on plates:

<<results=hide>>=
(positions <- find_positions(names))
@

This yields a nested list containing two-column matrices with plate names in the first and well coordinates in the second column.
References to external data resources for each substrate name can be obtained using \code{substrate\_info()}:

<<results=hide>>=
(subst.info <- substrate_info(names))
@

By default this yields CAS numbers, but KEGG and Metacyc IDs have also been collected for the majority of the substrates.
Another use of \code{substrate\_info()} is to convert substrate names to lower case but protecting name components such as abbreviations or chemical symbols.
See the manual for further details.


\subsection[Global settings]{Global settings}\label{application settings}

It is possible to modify settings that have an effect on multiple functions and/or on frequently used arguments globally using \code{opm\_opt()}.
It is checked that the novel values inherit from the same class(es) than the old ones.
See the manual for details.


\section[Discussion and conclusion]{Discussion and conclusion}

The high-dimensional sets of longitudinal data collected by the OmniLog\textregistered \ PM system call for fast and easily applicable (and extendable) data organisation and analysis facilities.
The here presented \pkg{opm} package for the free statistical software \proglang{R} \citep{r} features not only the calculation of aggregated values (curve parameters) including their (bootstrapped) confidence intervals, but also provides a rather complete infrastructure for the management of raw kinetic values and aggregated curve parameters together with any kind of meta-information of relevance for the user.
The analysis toolbox of the package includes the implementation of a fully automated estimation of whether respiration kinetics should be classified as either a ``positive'' or ``negative'' (absent) physiological reaction.
This dichotomization is apparently of high interest to many users of the OmniLog\textregistered \ PM system but would apparently be extremely biased as long as thresholds are chosen ad hoc and by eye.
(Users should nevertheless be aware that loss of information is inherent to discretizing continuous data.)
The \pkg{opm} package enables the user to produce highly informative and specialized graphical outputs from both the raw kinetic data as well as the curve parameter estimates.
In combination with the functionality for annotating the data with meta-information and then selecting subsets of the data, straightforward analyses regarding specific analytical questions can be performed without the need to invoke other \proglang{R} packages.

But since the design of the \pkg{opm} objects is not intended to be limited to specific analysis frameworks, the \pkg{opm} package works as a data containment providing well organized and comprehensive PM data for further, more specialized analyses using methods from different \proglang{R} packages or other third party software tools including phylogeny software.
The generation of \proglang{S4} objects featuring a rich set of methods as containers for either single or multiple OmniLog\textregistered \ PM plates enables not only the transfer of raw kinetic data into \proglang{R} but also eases their further processing with, for example, other \proglang{R} packages.
The complex data bundles can also be exported in \proglang{YAML} format (www.yaml.org), which is a human-readable data serialization format that can be read by most common programming languages and facilitates fast and easy data exchange between laboratories.

These features render the \pkg{opm} package to be the first comprehensive toolbox for the management and a broad range of analyses of OmniLog\textregistered \ PM data.
Its usage requires some familiarity with \proglang{R}, but is otherwise intuitive and straightforward also for biologists who are not used to command-line based software.

An enhancement of the \pkg{opm} package would be to include much more precomputed information about the substrates, thus greatly facilitating data arrangement and hypothesis testing.
At the moment only the translation of well coordinates to substrate names is provided, as well as access to CAS, KEGG and Metacyc IDs for the majority of the substrates.
More substrate information could be integrated into the package, particularly for arranging the substrate into groups, thus easing testing of phenotypic hypotheses.

Potential for improvement can also be seen in the spline estimation and parameter calculation in the data-aggregation step.
One main issue in the spline-fitting procedure is the selection of suitable smoothing parameters.
The here presented methods provide a basic framework for this based on methods from the \pkg{grofit} package, but could also be improved by the incorporation of approaches to the selection of smoothing parameters \textit{via} cross-validation \citep{eilers}, generalized cross-validation \citep{craven} and application of information criteria like AIC or BIC \citep{eilers} into the fitting procedure \citep{vaas}.
Last but not least, it might also be useful to provide functionality for a direct cross-talk between \pkg{opm} and database management systems.
The current version is entirely file-based, and whereas powerful selection mechanisms for both input files and container objects for previously imported PM plates have already been implemented, future version could directly include database access.
In the meantime, however, the output \proglang{YAML} format is likely to facilitate the quick establishment of third-party software for importing PM data into a database.

To summarize, we are convinced that the \pkg{opm} package already enables the users to analyse OmniLog\textregistered \ PM data in rather unlimited exploratory directions.


\section[Acknowledgements]{Acknowledgements}

We thank Barry Bochner (BIOLOG Inc.), John Kirkish (BIOLOG Inc.), Andre Chouankam (BIOLOG Inc.), Jan Meier-Kolthoff (DSMZ), and Stefan Ehrentraut (DSMZ) for helpful advice, as well as Victoria Michael (DSMZ) for technical support.
This work was supported by the German Research Foundation (DFG) SFB/TRR 51 and by the Microme project within the Framework 7 programme of the European Commission, which is gratefully acknowledged.
JS gratefully acknowledges his support by DFG grant SI 1352/1-2.


\bibliography{opm}


\end{document}
